{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae672cc",
   "metadata": {},
   "source": [
    "## Contents\n",
    "-  [1.Importing Libraries](#1)\n",
    "-  [2.Dataset Managament](#2)\n",
    "    -  [2.1.Downloading , Extracting and Spliting Dataset](#2.1)\n",
    "    -  [2.2.Pytorch Dataset](#2.2)\n",
    "    -  [2.3.Pytorch DataLoaders](#2.3)\n",
    "-  [3.Initializing pre-trained model](#3)\n",
    "-  [4.Training](#4)\n",
    "-  [5.Plotting Graphs](#5)\n",
    "    -  [5.1.Plotting Loss vs Epoch](#5.1)\n",
    "    -  [5.2.Plotting Accuracy vs Epoch](#5.2)\n",
    "-  [6.Loading and Testing](#6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1982a5b",
   "metadata": {},
   "source": [
    "### 1.Importing Libraries <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03af444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset as Dataset\n",
    "from torch.utils.data import DataLoader as DataLoader\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import wget\n",
    "import tarfile\n",
    "import os\n",
    "import math\n",
    "import torchvision.transforms as T\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f91e92",
   "metadata": {},
   "source": [
    "### 2.Dataset Managament <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf41f82d",
   "metadata": {},
   "source": [
    "#### 1.Downloading , Extracting and Spliting Dataset <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75f9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_url = 'http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar'\n",
    "#check if the zipfile already exists\n",
    "\n",
    "if \"images.tar\" not in os.listdir() :\n",
    "    wget.download('http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar')\n",
    "    \n",
    "#check if the zip file is extracted\n",
    "\n",
    "if len(os.listdir(\"data/\"))== 0:\n",
    "\n",
    "    # open file\n",
    "    file = tarfile.open('images.tar')\n",
    "  \n",
    "    # extracting file\n",
    "    file.extractall('data/')\n",
    "    print(\"Extraction completed\")\n",
    "    file.close()\n",
    "     \n",
    "    # creating train , test and validation folders\n",
    "    path = \"data/\"\n",
    "    classes = os.listdir(path+'/images')\n",
    "    os.mkdir(\"data/train\")\n",
    "    os.mkdir(\"data/test\")\n",
    "    os.mkdir(\"data/validation\")\n",
    "            \n",
    "    for i in classes:\n",
    "        os.mkdir('data/train/'+i)\n",
    "        os.mkdir('data/test/'+i)\n",
    "        os.mkdir('data/validation/'+i)\n",
    "    \n",
    "    # (class_n , images_name) storing all the image name/location\n",
    "    images_path = []\n",
    "    for i in classes:\n",
    "        temp = []\n",
    "        for j in os.listdir(path+'images/'+i+'/'):\n",
    "            temp.append(j)\n",
    "        images_path.append(temp)\n",
    "    \n",
    "    # splitting train , test and validation images for each classes\n",
    "    \n",
    "    train = 0.9\n",
    "    test = 0.075\n",
    "    validatiation = 0.025\n",
    "\n",
    "    train_img_path = []\n",
    "    test_img_path = []\n",
    "    validation_img_path = []\n",
    "\n",
    "    for i in images_path:\n",
    "        n = len(i)\n",
    "        train_n = int(train*n)\n",
    "        test_n = int(test*n)\n",
    "        validation_n = n - train_n - test_n\n",
    "    \n",
    "        train_img_path.append(i[:train_n])\n",
    "        test_img_path.append(i[train_n:test_n+train_n])\n",
    "        validation_img_path.append(i[train_n+test_n:])\n",
    "    \n",
    "    # Moving corresponding images to the train , test and validation folders\n",
    "\n",
    "    for i,class_name in zip(train_img_path,classes):\n",
    "        for j in i:\n",
    "            os.rename(path+\"Images/\"+class_name+\"/\"+j, path+\"train/\"+class_name+\"/\"+j)\n",
    "        \n",
    "    for i,class_name in zip(test_img_path,classes):\n",
    "        for j in i:\n",
    "            os.rename(path+\"Images/\"+class_name+\"/\"+j, path+\"test/\"+class_name+\"/\"+j)\n",
    "        \n",
    "    for i,class_name in zip(validation_img_path,classes):\n",
    "        for j in i:\n",
    "            os.rename(path+\"Images/\"+class_name+\"/\"+j, path+\"validation/\"+class_name+\"/\"+j)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fea45",
   "metadata": {},
   "source": [
    "#### 2.Pytorch Dataset <a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdf3ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests , io\n",
    "import os\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self , type = 'train' , transform = 'T.Resize((256,256))'):\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        self.transform = transform\n",
    "        self.n_classes = len(os.listdir('data/train/'))\n",
    "        if type == 'train':\n",
    "            path = 'data/train/'\n",
    "            classes = os.listdir(path)\n",
    "            for i in range(len(classes)):\n",
    "                for j in os.listdir(path+classes[i]):\n",
    "                    self.x.append(path+classes[i]+'/'+j)\n",
    "                    self.y.append(i)\n",
    "\n",
    "        if type == 'test':\n",
    "            path = 'data/test/'\n",
    "            classes = os.listdir(path)\n",
    "            for i in range(len(classes)):\n",
    "                for j in os.listdir(path+classes[i]):\n",
    "                    self.x.append(path+classes[i]+'/'+j)\n",
    "                    self.y.append(i)\n",
    "                    \n",
    "        if type == 'validation':\n",
    "            path = 'data/validation/'\n",
    "            classes = os.listdir(path)\n",
    "            for i in range(len(classes)):\n",
    "                for j in os.listdir(path+classes[i]):\n",
    "                    self.x.append(path+classes[i]+'/'+j)\n",
    "                    self.y.append(i)\n",
    "                    \n",
    "        self.len = len(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        image = self.transform(Image.fromarray(np.asarray(Image.open(self.x[i]))[:,:,:3]))\n",
    "        x = torch.FloatTensor(np.asarray(image))\n",
    "        return x,torch.LongTensor([self.y[i]])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387969b1",
   "metadata": {},
   "source": [
    "#### 3.Pytorch DataLoaders <a class=\"anchor\" id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdb4319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "classes = os.listdir('data/train/')\n",
    "n_classes = len(classes)\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.RandomRotation(degrees=30),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transforms_test = T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train',transform =transforms_train )\n",
    "test_dataset = Dataset('test',transform =transforms_test )\n",
    "validation_dataset = Dataset('validation',transform =transforms_test )\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = batch_size , shuffle = True )\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b581a93d",
   "metadata": {},
   "source": [
    "### 3.Initializing pre-trained model <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5986f6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False                     # Freezing the model parameters that isn't required to be trained\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, n_classes)\n",
    "\n",
    "# for name,child in model.named_children():\n",
    "#     print(name,child)\n",
    "\n",
    "# class extended_model(nn.Module):\n",
    "#     def __init__(self,pretrained_model):\n",
    "#         super().__init__()\n",
    "#         self.pretrained_model = pretrained_model     #Pretrained models can be extended this way\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.pretrained_model(x)\n",
    "#         x = F.softmax(x)\n",
    "#         return x\n",
    "\n",
    "# model = extended_model(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1223bae",
   "metadata": {},
   "source": [
    "### 4.Training <a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f115ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epoch = 15\n",
    "train_dataset_size = train_dataset.__len__()\n",
    "validation_dataset_size = validation_dataset.__len__()\n",
    "train_n_minibatches = train_dataloader.__len__()\n",
    "validation_n_minibatches = validation_dataloader.__len__()\n",
    "\n",
    "device = 'cuda'\n",
    "model.to(device)\n",
    "loss_history = [[],[]] #[[train], [validation]]\n",
    "accuracy_history = [[],[]] #[[train], [validation]]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(),lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029860ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS for EPOCH 1 BATCH 0/1155 TRAIN LOSS : 4.9398908615112305 TRAIN ACCURACY : 0.0 VALIDATION LOSS : 4.8720204591751095 VALIDATION ACCURACY : 0.78125\n",
      "LOSS for EPOCH 1 BATCH 100/1155 TRAIN LOSS : 4.5158772468566895 TRAIN ACCURACY : 0.0 VALIDATION LOSS : 4.301684910058976 VALIDATION ACCURACY : 13.28125\n",
      "LOSS for EPOCH 1 BATCH 200/1155 TRAIN LOSS : 3.897536277770996 TRAIN ACCURACY : 31.25 VALIDATION LOSS : 3.823131251335144 VALIDATION ACCURACY : 24.0625\n",
      "LOSS for EPOCH 1 BATCH 300/1155 TRAIN LOSS : 3.7879326343536377 TRAIN ACCURACY : 25.0 VALIDATION LOSS : 3.39531369805336 VALIDATION ACCURACY : 38.90625\n",
      "LOSS for EPOCH 1 BATCH 400/1155 TRAIN LOSS : 3.360856294631958 TRAIN ACCURACY : 25.0 VALIDATION LOSS : 3.006963860988617 VALIDATION ACCURACY : 49.6875\n",
      "LOSS for EPOCH 1 BATCH 500/1155 TRAIN LOSS : 2.779902935028076 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 2.6652338802814484 VALIDATION ACCURACY : 52.65625\n",
      "LOSS for EPOCH 1 BATCH 600/1155 TRAIN LOSS : 2.6920766830444336 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 2.3512704759836196 VALIDATION ACCURACY : 62.96875\n",
      "LOSS for EPOCH 1 BATCH 700/1155 TRAIN LOSS : 2.621244430541992 TRAIN ACCURACY : 37.5 VALIDATION LOSS : 2.1202228337526323 VALIDATION ACCURACY : 63.75\n",
      "LOSS for EPOCH 1 BATCH 800/1155 TRAIN LOSS : 2.0230202674865723 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 1.9982905447483064 VALIDATION ACCURACY : 64.375\n",
      "LOSS for EPOCH 1 BATCH 900/1155 TRAIN LOSS : 2.194992780685425 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 1.7668592512607575 VALIDATION ACCURACY : 68.28125\n",
      "LOSS for EPOCH 1 BATCH 1000/1155 TRAIN LOSS : 2.0227649211883545 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 1.687339472770691 VALIDATION ACCURACY : 70.9375\n",
      "LOSS for EPOCH 1 BATCH 1100/1155 TRAIN LOSS : 1.787841558456421 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 1.5621209651231767 VALIDATION ACCURACY : 70.0\n",
      "---------------------------------------EPOCH 1-------------------------------------------\n",
      "Loss for EPOCH 1  TRAIN LOSS : 2.9847891671316966 TRAIN ACCURACY : 43.90151515151515\n",
      "VALIDATION LOSS for EPOCH 1 : 2.60713010403243 VALIDATION ACCURACY : 52.62784090909091\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 2 BATCH 0/1155 TRAIN LOSS : 2.006286144256592 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 1.4770140066742896 VALIDATION ACCURACY : 70.3125\n",
      "LOSS for EPOCH 2 BATCH 100/1155 TRAIN LOSS : 1.909467339515686 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 1.4274109080433846 VALIDATION ACCURACY : 71.875\n",
      "LOSS for EPOCH 2 BATCH 200/1155 TRAIN LOSS : 1.5807429552078247 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 1.3682128012180328 VALIDATION ACCURACY : 71.5625\n",
      "LOSS for EPOCH 2 BATCH 300/1155 TRAIN LOSS : 2.1296210289001465 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 1.3330257833003998 VALIDATION ACCURACY : 73.4375\n",
      "LOSS for EPOCH 2 BATCH 400/1155 TRAIN LOSS : 1.3567606210708618 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 1.2361983790993691 VALIDATION ACCURACY : 73.90625\n",
      "LOSS for EPOCH 2 BATCH 500/1155 TRAIN LOSS : 1.6151244640350342 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 1.2186791971325874 VALIDATION ACCURACY : 73.4375\n",
      "LOSS for EPOCH 2 BATCH 600/1155 TRAIN LOSS : 1.5608528852462769 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 1.1376478612422942 VALIDATION ACCURACY : 76.5625\n",
      "LOSS for EPOCH 2 BATCH 700/1155 TRAIN LOSS : 1.965158462524414 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 1.127134908735752 VALIDATION ACCURACY : 74.53125\n",
      "LOSS for EPOCH 2 BATCH 800/1155 TRAIN LOSS : 1.3106034994125366 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 1.1001459460705518 VALIDATION ACCURACY : 76.09375\n",
      "LOSS for EPOCH 2 BATCH 900/1155 TRAIN LOSS : 1.848367691040039 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 1.0365468688309192 VALIDATION ACCURACY : 74.84375\n",
      "LOSS for EPOCH 2 BATCH 1000/1155 TRAIN LOSS : 0.9937036037445068 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 1.0278104677796365 VALIDATION ACCURACY : 75.3125\n",
      "LOSS for EPOCH 2 BATCH 1100/1155 TRAIN LOSS : 1.5682040452957153 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 1.028578033298254 VALIDATION ACCURACY : 75.3125\n",
      "---------------------------------------EPOCH 2-------------------------------------------\n",
      "Loss for EPOCH 2  TRAIN LOSS : 1.5412734832598534 TRAIN ACCURACY : 68.50468975468975\n",
      "VALIDATION LOSS for EPOCH 2 : 1.1855810140682892 VALIDATION ACCURACY : 74.26136363636364\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 3 BATCH 0/1155 TRAIN LOSS : 1.1602778434753418 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 1.0246165245771408 VALIDATION ACCURACY : 76.71875\n",
      "LOSS for EPOCH 3 BATCH 100/1155 TRAIN LOSS : 1.1258773803710938 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.9840047016739846 VALIDATION ACCURACY : 76.40625\n",
      "LOSS for EPOCH 3 BATCH 200/1155 TRAIN LOSS : 1.7180287837982178 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.961737684905529 VALIDATION ACCURACY : 75.3125\n",
      "LOSS for EPOCH 3 BATCH 300/1155 TRAIN LOSS : 0.8538181781768799 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.9314154483377933 VALIDATION ACCURACY : 77.96875\n",
      "LOSS for EPOCH 3 BATCH 400/1155 TRAIN LOSS : 1.4645370244979858 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.970685064047575 VALIDATION ACCURACY : 73.90625\n",
      "LOSS for EPOCH 3 BATCH 500/1155 TRAIN LOSS : 1.2975813150405884 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.9409107722342014 VALIDATION ACCURACY : 75.3125\n",
      "LOSS for EPOCH 3 BATCH 600/1155 TRAIN LOSS : 1.154891848564148 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.9081244356930256 VALIDATION ACCURACY : 76.40625\n",
      "LOSS for EPOCH 3 BATCH 700/1155 TRAIN LOSS : 1.5347446203231812 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.915603480860591 VALIDATION ACCURACY : 77.5\n",
      "LOSS for EPOCH 3 BATCH 800/1155 TRAIN LOSS : 1.2212283611297607 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.8889554001390934 VALIDATION ACCURACY : 75.78125\n",
      "LOSS for EPOCH 3 BATCH 900/1155 TRAIN LOSS : 1.3263804912567139 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.8585342492908239 VALIDATION ACCURACY : 76.71875\n",
      "LOSS for EPOCH 3 BATCH 1000/1155 TRAIN LOSS : 1.7190231084823608 TRAIN ACCURACY : 50.0 VALIDATION LOSS : 0.8438244387507439 VALIDATION ACCURACY : 77.03125\n",
      "LOSS for EPOCH 3 BATCH 1100/1155 TRAIN LOSS : 1.198310375213623 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.8571417801082134 VALIDATION ACCURACY : 77.1875\n",
      "---------------------------------------EPOCH 3-------------------------------------------\n",
      "Loss for EPOCH 3  TRAIN LOSS : 1.2195566221749112 TRAIN ACCURACY : 71.57106782106783\n",
      "VALIDATION LOSS for EPOCH 3 : 0.9146306778219613 VALIDATION ACCURACY : 76.32102272727273\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 4 BATCH 0/1155 TRAIN LOSS : 1.6808987855911255 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.8727350875735282 VALIDATION ACCURACY : 77.03125\n",
      "LOSS for EPOCH 4 BATCH 100/1155 TRAIN LOSS : 0.9543750286102295 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.8679679110646248 VALIDATION ACCURACY : 76.25\n",
      "LOSS for EPOCH 4 BATCH 200/1155 TRAIN LOSS : 1.0800256729125977 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.837118311971426 VALIDATION ACCURACY : 77.8125\n",
      "LOSS for EPOCH 4 BATCH 300/1155 TRAIN LOSS : 0.97669517993927 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.8033314429223537 VALIDATION ACCURACY : 78.4375\n",
      "LOSS for EPOCH 4 BATCH 400/1155 TRAIN LOSS : 1.2974551916122437 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.8187482811510562 VALIDATION ACCURACY : 78.125\n",
      "LOSS for EPOCH 4 BATCH 500/1155 TRAIN LOSS : 1.2467197179794312 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.8475374475121498 VALIDATION ACCURACY : 76.09375\n",
      "LOSS for EPOCH 4 BATCH 600/1155 TRAIN LOSS : 1.1965875625610352 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.8228071935474872 VALIDATION ACCURACY : 75.625\n",
      "LOSS for EPOCH 4 BATCH 700/1155 TRAIN LOSS : 1.4003764390945435 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.787665544077754 VALIDATION ACCURACY : 77.8125\n",
      "LOSS for EPOCH 4 BATCH 800/1155 TRAIN LOSS : 1.0225944519042969 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.8151174508035183 VALIDATION ACCURACY : 77.96875\n",
      "LOSS for EPOCH 4 BATCH 900/1155 TRAIN LOSS : 1.4638339281082153 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.8031868446618319 VALIDATION ACCURACY : 77.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS for EPOCH 4 BATCH 1000/1155 TRAIN LOSS : 1.0512369871139526 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.789505410194397 VALIDATION ACCURACY : 77.65625\n",
      "LOSS for EPOCH 4 BATCH 1100/1155 TRAIN LOSS : 1.410088062286377 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.7944278564304114 VALIDATION ACCURACY : 78.125\n",
      "---------------------------------------EPOCH 4-------------------------------------------\n",
      "Loss for EPOCH 4  TRAIN LOSS : 1.0864785180979477 TRAIN ACCURACY : 73.06818181818181\n",
      "VALIDATION LOSS for EPOCH 4 : 0.8170376085760919 VALIDATION ACCURACY : 77.40056818181819\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 5 BATCH 0/1155 TRAIN LOSS : 1.3517186641693115 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.7964604862034321 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 5 BATCH 100/1155 TRAIN LOSS : 1.0250182151794434 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.788381428271532 VALIDATION ACCURACY : 77.1875\n",
      "LOSS for EPOCH 5 BATCH 200/1155 TRAIN LOSS : 1.225618839263916 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.7859917625784874 VALIDATION ACCURACY : 77.34375\n",
      "LOSS for EPOCH 5 BATCH 300/1155 TRAIN LOSS : 1.371606469154358 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7858589150011539 VALIDATION ACCURACY : 77.65625\n",
      "LOSS for EPOCH 5 BATCH 400/1155 TRAIN LOSS : 0.9660362005233765 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7702908597886562 VALIDATION ACCURACY : 78.59375\n",
      "LOSS for EPOCH 5 BATCH 500/1155 TRAIN LOSS : 0.9652491211891174 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.7855268612504005 VALIDATION ACCURACY : 76.40625\n",
      "LOSS for EPOCH 5 BATCH 600/1155 TRAIN LOSS : 0.9269811511039734 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7527942229062319 VALIDATION ACCURACY : 79.21875\n",
      "LOSS for EPOCH 5 BATCH 700/1155 TRAIN LOSS : 1.0171862840652466 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7546547297388315 VALIDATION ACCURACY : 78.125\n",
      "LOSS for EPOCH 5 BATCH 800/1155 TRAIN LOSS : 1.220077633857727 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7566196873784066 VALIDATION ACCURACY : 78.59375\n",
      "LOSS for EPOCH 5 BATCH 900/1155 TRAIN LOSS : 0.7187924385070801 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7419654715806245 VALIDATION ACCURACY : 79.21875\n",
      "LOSS for EPOCH 5 BATCH 1000/1155 TRAIN LOSS : 1.6333203315734863 TRAIN ACCURACY : 50.0 VALIDATION LOSS : 0.7686582546681165 VALIDATION ACCURACY : 77.8125\n",
      "LOSS for EPOCH 5 BATCH 1100/1155 TRAIN LOSS : 0.928846001625061 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7603661235421896 VALIDATION ACCURACY : 77.65625\n",
      "---------------------------------------EPOCH 5-------------------------------------------\n",
      "Loss for EPOCH 5  TRAIN LOSS : 1.0024199037324815 TRAIN ACCURACY : 73.94660894660895\n",
      "VALIDATION LOSS for EPOCH 5 : 0.7682825742458755 VALIDATION ACCURACY : 77.98295454545455\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 6 BATCH 0/1155 TRAIN LOSS : 0.8624404072761536 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7373405829071998 VALIDATION ACCURACY : 78.4375\n",
      "LOSS for EPOCH 6 BATCH 100/1155 TRAIN LOSS : 0.8244829773902893 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.7445185527205467 VALIDATION ACCURACY : 78.59375\n",
      "LOSS for EPOCH 6 BATCH 200/1155 TRAIN LOSS : 1.0642012357711792 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7319817461073399 VALIDATION ACCURACY : 78.4375\n",
      "LOSS for EPOCH 6 BATCH 300/1155 TRAIN LOSS : 0.971397876739502 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7435309000313282 VALIDATION ACCURACY : 78.4375\n",
      "LOSS for EPOCH 6 BATCH 400/1155 TRAIN LOSS : 0.895652711391449 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7369998443871737 VALIDATION ACCURACY : 79.84375\n",
      "LOSS for EPOCH 6 BATCH 500/1155 TRAIN LOSS : 1.1313836574554443 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.7253825113177299 VALIDATION ACCURACY : 78.90625\n",
      "LOSS for EPOCH 6 BATCH 600/1155 TRAIN LOSS : 0.8303892612457275 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7297336250543595 VALIDATION ACCURACY : 79.6875\n",
      "LOSS for EPOCH 6 BATCH 700/1155 TRAIN LOSS : 0.7862974405288696 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7364794660359621 VALIDATION ACCURACY : 78.4375\n",
      "LOSS for EPOCH 6 BATCH 800/1155 TRAIN LOSS : 0.5856738686561584 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7291542571038008 VALIDATION ACCURACY : 79.6875\n",
      "LOSS for EPOCH 6 BATCH 900/1155 TRAIN LOSS : 1.2849466800689697 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.745162795484066 VALIDATION ACCURACY : 77.96875\n",
      "LOSS for EPOCH 6 BATCH 1000/1155 TRAIN LOSS : 0.6518537402153015 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 0.7103955838829279 VALIDATION ACCURACY : 79.21875\n",
      "LOSS for EPOCH 6 BATCH 1100/1155 TRAIN LOSS : 0.8406201004981995 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7009263675659895 VALIDATION ACCURACY : 79.6875\n",
      "---------------------------------------EPOCH 6-------------------------------------------\n",
      "Loss for EPOCH 6  TRAIN LOSS : 0.946153436646317 TRAIN ACCURACY : 74.54545454545455\n",
      "VALIDATION LOSS for EPOCH 6 : 0.7303877863355658 VALIDATION ACCURACY : 78.99147727272727\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 7 BATCH 0/1155 TRAIN LOSS : 0.5349025130271912 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.716184799745679 VALIDATION ACCURACY : 79.375\n",
      "LOSS for EPOCH 7 BATCH 100/1155 TRAIN LOSS : 1.5404971837997437 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.7454566122964025 VALIDATION ACCURACY : 79.84375\n",
      "LOSS for EPOCH 7 BATCH 200/1155 TRAIN LOSS : 0.8935989737510681 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7534777969121933 VALIDATION ACCURACY : 77.96875\n",
      "LOSS for EPOCH 7 BATCH 300/1155 TRAIN LOSS : 0.707880437374115 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7096460707485676 VALIDATION ACCURACY : 79.53125\n",
      "LOSS for EPOCH 7 BATCH 400/1155 TRAIN LOSS : 1.0436437129974365 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7195682540535927 VALIDATION ACCURACY : 78.75\n",
      "LOSS for EPOCH 7 BATCH 500/1155 TRAIN LOSS : 1.0532596111297607 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.7171741236001253 VALIDATION ACCURACY : 77.65625\n",
      "LOSS for EPOCH 7 BATCH 600/1155 TRAIN LOSS : 0.8448835611343384 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6938003309071064 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 7 BATCH 700/1155 TRAIN LOSS : 0.5847592353820801 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 0.7232805173844099 VALIDATION ACCURACY : 78.75\n",
      "LOSS for EPOCH 7 BATCH 800/1155 TRAIN LOSS : 0.6728162169456482 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6990515984594822 VALIDATION ACCURACY : 78.28125\n",
      "LOSS for EPOCH 7 BATCH 900/1155 TRAIN LOSS : 0.6645872592926025 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7096209522336722 VALIDATION ACCURACY : 79.21875\n",
      "LOSS for EPOCH 7 BATCH 1000/1155 TRAIN LOSS : 1.0345934629440308 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.7019246717914939 VALIDATION ACCURACY : 80.625\n",
      "LOSS for EPOCH 7 BATCH 1100/1155 TRAIN LOSS : 0.6616593599319458 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 0.7135533854365349 VALIDATION ACCURACY : 79.6875\n",
      "---------------------------------------EPOCH 7-------------------------------------------\n",
      "Loss for EPOCH 7  TRAIN LOSS : 0.922002546689211 TRAIN ACCURACY : 74.6518759018759\n",
      "VALIDATION LOSS for EPOCH 7 : 0.716959483074871 VALIDATION ACCURACY : 79.13352272727273\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 8 BATCH 0/1155 TRAIN LOSS : 0.8330131769180298 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.7054564449936152 VALIDATION ACCURACY : 78.59375\n",
      "LOSS for EPOCH 8 BATCH 100/1155 TRAIN LOSS : 1.3073755502700806 TRAIN ACCURACY : 50.0 VALIDATION LOSS : 0.67769146990031 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 8 BATCH 200/1155 TRAIN LOSS : 0.8988023996353149 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7179329257458449 VALIDATION ACCURACY : 77.65625\n",
      "LOSS for EPOCH 8 BATCH 300/1155 TRAIN LOSS : 0.9560339450836182 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.716049287095666 VALIDATION ACCURACY : 80.46875\n",
      "LOSS for EPOCH 8 BATCH 400/1155 TRAIN LOSS : 1.1009812355041504 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6981196597218513 VALIDATION ACCURACY : 78.28125\n",
      "LOSS for EPOCH 8 BATCH 500/1155 TRAIN LOSS : 1.3260343074798584 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.7048201134428382 VALIDATION ACCURACY : 78.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS for EPOCH 8 BATCH 600/1155 TRAIN LOSS : 0.9198978543281555 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7139200676232577 VALIDATION ACCURACY : 77.96875\n",
      "LOSS for EPOCH 8 BATCH 700/1155 TRAIN LOSS : 0.9367157220840454 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7041053915396333 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 8 BATCH 800/1155 TRAIN LOSS : 0.6045886278152466 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6682798281311989 VALIDATION ACCURACY : 80.3125\n",
      "LOSS for EPOCH 8 BATCH 900/1155 TRAIN LOSS : 0.8025904893875122 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.7126041483134031 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 8 BATCH 1000/1155 TRAIN LOSS : 1.6632215976715088 TRAIN ACCURACY : 43.75 VALIDATION LOSS : 0.7029670726507902 VALIDATION ACCURACY : 79.6875\n",
      "LOSS for EPOCH 8 BATCH 1100/1155 TRAIN LOSS : 0.8616142272949219 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6856801142916084 VALIDATION ACCURACY : 80.3125\n",
      "---------------------------------------EPOCH 8-------------------------------------------\n",
      "Loss for EPOCH 8  TRAIN LOSS : 0.8959343577360178 TRAIN ACCURACY : 75.00541125541126\n",
      "VALIDATION LOSS for EPOCH 8 : 0.7001972798596728 VALIDATION ACCURACY : 79.41761363636364\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 9 BATCH 0/1155 TRAIN LOSS : 1.3235315084457397 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.676853377558291 VALIDATION ACCURACY : 79.6875\n",
      "LOSS for EPOCH 9 BATCH 100/1155 TRAIN LOSS : 0.5307357907295227 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6938519805669785 VALIDATION ACCURACY : 80.3125\n",
      "LOSS for EPOCH 9 BATCH 200/1155 TRAIN LOSS : 0.48209625482559204 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 0.7029629787430167 VALIDATION ACCURACY : 79.53125\n",
      "LOSS for EPOCH 9 BATCH 300/1155 TRAIN LOSS : 0.2239501178264618 TRAIN ACCURACY : 100.0 VALIDATION LOSS : 0.6776001833379268 VALIDATION ACCURACY : 80.46875\n",
      "LOSS for EPOCH 9 BATCH 400/1155 TRAIN LOSS : 1.2190263271331787 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.6834239859133959 VALIDATION ACCURACY : 78.75\n",
      "LOSS for EPOCH 9 BATCH 500/1155 TRAIN LOSS : 0.6575433611869812 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6934341743588448 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 9 BATCH 600/1155 TRAIN LOSS : 0.6064943671226501 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6819946315139532 VALIDATION ACCURACY : 79.0625\n",
      "LOSS for EPOCH 9 BATCH 700/1155 TRAIN LOSS : 1.3354603052139282 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.68719312697649 VALIDATION ACCURACY : 78.28125\n",
      "LOSS for EPOCH 9 BATCH 800/1155 TRAIN LOSS : 0.7645644545555115 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6752288559451699 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 9 BATCH 900/1155 TRAIN LOSS : 0.8779962658882141 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6951772678643465 VALIDATION ACCURACY : 79.21875\n",
      "LOSS for EPOCH 9 BATCH 1000/1155 TRAIN LOSS : 0.6181033253669739 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6903960017487407 VALIDATION ACCURACY : 80.3125\n",
      "LOSS for EPOCH 9 BATCH 1100/1155 TRAIN LOSS : 1.0635325908660889 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.670626112818718 VALIDATION ACCURACY : 80.46875\n",
      "---------------------------------------EPOCH 9-------------------------------------------\n",
      "Loss for EPOCH 9  TRAIN LOSS : 0.8662820467701206 TRAIN ACCURACY : 75.96681096681095\n",
      "VALIDATION LOSS for EPOCH 9 : 0.6865353908897801 VALIDATION ACCURACY : 79.6875\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 10 BATCH 0/1155 TRAIN LOSS : 0.6519956588745117 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6750644706189632 VALIDATION ACCURACY : 79.6875\n",
      "LOSS for EPOCH 10 BATCH 100/1155 TRAIN LOSS : 0.6115490198135376 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6866748174652457 VALIDATION ACCURACY : 79.84375\n",
      "LOSS for EPOCH 10 BATCH 200/1155 TRAIN LOSS : 0.6758788228034973 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6887226577848196 VALIDATION ACCURACY : 79.0625\n",
      "LOSS for EPOCH 10 BATCH 300/1155 TRAIN LOSS : 0.7026517987251282 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.6893563069403171 VALIDATION ACCURACY : 80.78125\n",
      "LOSS for EPOCH 10 BATCH 400/1155 TRAIN LOSS : 1.0958155393600464 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6799491599202157 VALIDATION ACCURACY : 79.0625\n",
      "LOSS for EPOCH 10 BATCH 500/1155 TRAIN LOSS : 0.8784212470054626 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.6900940738618374 VALIDATION ACCURACY : 80.3125\n",
      "LOSS for EPOCH 10 BATCH 600/1155 TRAIN LOSS : 0.5664835572242737 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6713655130937696 VALIDATION ACCURACY : 80.46875\n",
      "LOSS for EPOCH 10 BATCH 700/1155 TRAIN LOSS : 0.6555699110031128 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6834121597930789 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 10 BATCH 800/1155 TRAIN LOSS : 0.6964966058731079 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6706118213012815 VALIDATION ACCURACY : 79.0625\n",
      "LOSS for EPOCH 10 BATCH 900/1155 TRAIN LOSS : 0.4165738523006439 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 0.6759016782976687 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 10 BATCH 1000/1155 TRAIN LOSS : 0.6228050589561462 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6855072189122439 VALIDATION ACCURACY : 80.46875\n",
      "LOSS for EPOCH 10 BATCH 1100/1155 TRAIN LOSS : 0.4917981028556824 TRAIN ACCURACY : 100.0 VALIDATION LOSS : 0.6620858088135719 VALIDATION ACCURACY : 80.15625\n",
      "---------------------------------------EPOCH 10-------------------------------------------\n",
      "Loss for EPOCH 10  TRAIN LOSS : 0.8499552210172018 TRAIN ACCURACY : 76.06601731601732\n",
      "VALIDATION LOSS for EPOCH 10 : 0.6803346560167318 VALIDATION ACCURACY : 79.95738636363636\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 11 BATCH 0/1155 TRAIN LOSS : 0.5256766676902771 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6920965977013112 VALIDATION ACCURACY : 80.625\n",
      "LOSS for EPOCH 11 BATCH 100/1155 TRAIN LOSS : 0.7812872529029846 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.662637846916914 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 11 BATCH 200/1155 TRAIN LOSS : 1.0655643939971924 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6803942255675792 VALIDATION ACCURACY : 78.4375\n",
      "LOSS for EPOCH 11 BATCH 300/1155 TRAIN LOSS : 0.8838441371917725 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6699271319434047 VALIDATION ACCURACY : 79.375\n",
      "LOSS for EPOCH 11 BATCH 400/1155 TRAIN LOSS : 0.6498773694038391 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.679792121425271 VALIDATION ACCURACY : 80.3125\n",
      "LOSS for EPOCH 11 BATCH 500/1155 TRAIN LOSS : 1.1283351182937622 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.7030130680650473 VALIDATION ACCURACY : 80.46875\n",
      "LOSS for EPOCH 11 BATCH 600/1155 TRAIN LOSS : 0.8500282168388367 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6791182976216078 VALIDATION ACCURACY : 80.625\n",
      "LOSS for EPOCH 11 BATCH 700/1155 TRAIN LOSS : 1.0803290605545044 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6623006004840135 VALIDATION ACCURACY : 79.21875\n",
      "LOSS for EPOCH 11 BATCH 800/1155 TRAIN LOSS : 1.1472114324569702 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.6670452381484211 VALIDATION ACCURACY : 79.6875\n",
      "LOSS for EPOCH 11 BATCH 900/1155 TRAIN LOSS : 0.6734707355499268 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6710824755951762 VALIDATION ACCURACY : 79.84375\n",
      "LOSS for EPOCH 11 BATCH 1000/1155 TRAIN LOSS : 0.7655398845672607 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.663788678124547 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 11 BATCH 1100/1155 TRAIN LOSS : 0.9490852952003479 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6702864423394204 VALIDATION ACCURACY : 79.6875\n",
      "---------------------------------------EPOCH 11-------------------------------------------\n",
      "Loss for EPOCH 11  TRAIN LOSS : 0.8340996531697062 TRAIN ACCURACY : 76.03174603174602\n",
      "VALIDATION LOSS for EPOCH 11 : 0.6735805569301274 VALIDATION ACCURACY : 79.80113636363636\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 12 BATCH 0/1155 TRAIN LOSS : 0.8912426233291626 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6898762298747897 VALIDATION ACCURACY : 79.53125\n",
      "LOSS for EPOCH 12 BATCH 100/1155 TRAIN LOSS : 0.3883834481239319 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 0.6827543999999761 VALIDATION ACCURACY : 79.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS for EPOCH 12 BATCH 200/1155 TRAIN LOSS : 0.47091010212898254 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6439166467636823 VALIDATION ACCURACY : 80.78125\n",
      "LOSS for EPOCH 12 BATCH 300/1155 TRAIN LOSS : 0.8135905265808105 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6906504213809967 VALIDATION ACCURACY : 79.375\n",
      "LOSS for EPOCH 12 BATCH 400/1155 TRAIN LOSS : 1.1357238292694092 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6768831703811884 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 12 BATCH 500/1155 TRAIN LOSS : 0.9494622945785522 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6588226433843374 VALIDATION ACCURACY : 79.6875\n",
      "LOSS for EPOCH 12 BATCH 600/1155 TRAIN LOSS : 0.5908036231994629 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6745120696723461 VALIDATION ACCURACY : 78.90625\n",
      "LOSS for EPOCH 12 BATCH 700/1155 TRAIN LOSS : 0.9192638993263245 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6630229020491243 VALIDATION ACCURACY : 81.25\n",
      "LOSS for EPOCH 12 BATCH 800/1155 TRAIN LOSS : 0.6883998513221741 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6839921336621047 VALIDATION ACCURACY : 79.53125\n",
      "LOSS for EPOCH 12 BATCH 900/1155 TRAIN LOSS : 0.5125646591186523 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6899646917358041 VALIDATION ACCURACY : 79.375\n",
      "LOSS for EPOCH 12 BATCH 1000/1155 TRAIN LOSS : 0.6438033580780029 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.659993025381118 VALIDATION ACCURACY : 79.84375\n",
      "LOSS for EPOCH 12 BATCH 1100/1155 TRAIN LOSS : 0.8670405149459839 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.663288084231317 VALIDATION ACCURACY : 79.84375\n",
      "---------------------------------------EPOCH 12-------------------------------------------\n",
      "Loss for EPOCH 12  TRAIN LOSS : 0.8077772455814081 TRAIN ACCURACY : 76.82178932178931\n",
      "VALIDATION LOSS for EPOCH 12 : 0.6716181989674541 VALIDATION ACCURACY : 79.8153409090909\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 13 BATCH 0/1155 TRAIN LOSS : 0.5133692622184753 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6849622868001461 VALIDATION ACCURACY : 78.90625\n",
      "LOSS for EPOCH 13 BATCH 100/1155 TRAIN LOSS : 1.4163975715637207 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.6613719454035163 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 13 BATCH 200/1155 TRAIN LOSS : 0.5758203268051147 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6738934345543385 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 13 BATCH 300/1155 TRAIN LOSS : 1.2235242128372192 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.676875650882721 VALIDATION ACCURACY : 79.53125\n",
      "LOSS for EPOCH 13 BATCH 400/1155 TRAIN LOSS : 0.5608342289924622 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.677431470900774 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 13 BATCH 500/1155 TRAIN LOSS : 0.8373906016349792 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6455190991982818 VALIDATION ACCURACY : 80.625\n",
      "LOSS for EPOCH 13 BATCH 600/1155 TRAIN LOSS : 0.7448008060455322 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6604615604504943 VALIDATION ACCURACY : 80.78125\n",
      "LOSS for EPOCH 13 BATCH 700/1155 TRAIN LOSS : 0.5774372220039368 TRAIN ACCURACY : 93.75 VALIDATION LOSS : 0.6806165976449847 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 13 BATCH 800/1155 TRAIN LOSS : 0.8438926339149475 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6549405083060265 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 13 BATCH 900/1155 TRAIN LOSS : 1.1076385974884033 TRAIN ACCURACY : 50.0 VALIDATION LOSS : 0.6722230080515146 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 13 BATCH 1000/1155 TRAIN LOSS : 1.012170433998108 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6907289873808622 VALIDATION ACCURACY : 79.0625\n",
      "LOSS for EPOCH 13 BATCH 1100/1155 TRAIN LOSS : 0.7003908753395081 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6831252325326205 VALIDATION ACCURACY : 79.6875\n",
      "---------------------------------------EPOCH 13-------------------------------------------\n",
      "Loss for EPOCH 13  TRAIN LOSS : 0.8028811450257446 TRAIN ACCURACY : 76.87950937950937\n",
      "VALIDATION LOSS for EPOCH 13 : 0.670653408664194 VALIDATION ACCURACY : 80.01420454545455\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 14 BATCH 0/1155 TRAIN LOSS : 0.6739879846572876 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6703428769484162 VALIDATION ACCURACY : 79.0625\n",
      "LOSS for EPOCH 14 BATCH 100/1155 TRAIN LOSS : 0.5840998291969299 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6722474557347595 VALIDATION ACCURACY : 81.25\n",
      "LOSS for EPOCH 14 BATCH 200/1155 TRAIN LOSS : 0.4583069086074829 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6648654233664274 VALIDATION ACCURACY : 80.625\n",
      "LOSS for EPOCH 14 BATCH 300/1155 TRAIN LOSS : 0.6183758974075317 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6708005035296083 VALIDATION ACCURACY : 78.125\n",
      "LOSS for EPOCH 14 BATCH 400/1155 TRAIN LOSS : 0.7450286149978638 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6717906545847654 VALIDATION ACCURACY : 80.78125\n",
      "LOSS for EPOCH 14 BATCH 500/1155 TRAIN LOSS : 0.697956919670105 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6697118883952499 VALIDATION ACCURACY : 80.625\n",
      "LOSS for EPOCH 14 BATCH 600/1155 TRAIN LOSS : 0.5308992266654968 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6505040835589171 VALIDATION ACCURACY : 80.78125\n",
      "LOSS for EPOCH 14 BATCH 700/1155 TRAIN LOSS : 0.9810484647750854 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6623383384197951 VALIDATION ACCURACY : 80.3125\n",
      "LOSS for EPOCH 14 BATCH 800/1155 TRAIN LOSS : 0.8470388650894165 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6573404047638178 VALIDATION ACCURACY : 80.78125\n",
      "LOSS for EPOCH 14 BATCH 900/1155 TRAIN LOSS : 0.9982528686523438 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6527339385822415 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 14 BATCH 1000/1155 TRAIN LOSS : 0.5953606367111206 TRAIN ACCURACY : 81.25 VALIDATION LOSS : 0.6735959872603416 VALIDATION ACCURACY : 78.59375\n",
      "LOSS for EPOCH 14 BATCH 1100/1155 TRAIN LOSS : 1.0210169553756714 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6726487601175904 VALIDATION ACCURACY : 80.78125\n",
      "---------------------------------------EPOCH 14-------------------------------------------\n",
      "Loss for EPOCH 14  TRAIN LOSS : 0.7867128389867353 TRAIN ACCURACY : 77.41522366522368\n",
      "VALIDATION LOSS for EPOCH 14 : 0.665325221664865 VALIDATION ACCURACY : 80.24147727272727\n",
      "---------------------------------------------------------------------------------------------\n",
      "LOSS for EPOCH 15 BATCH 0/1155 TRAIN LOSS : 1.2528343200683594 TRAIN ACCURACY : 56.25 VALIDATION LOSS : 0.6669754765927791 VALIDATION ACCURACY : 78.59375\n",
      "LOSS for EPOCH 15 BATCH 100/1155 TRAIN LOSS : 0.49389657378196716 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6697742104530334 VALIDATION ACCURACY : 80.3125\n",
      "LOSS for EPOCH 15 BATCH 200/1155 TRAIN LOSS : 0.5660715103149414 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6539425691589713 VALIDATION ACCURACY : 80.46875\n",
      "LOSS for EPOCH 15 BATCH 300/1155 TRAIN LOSS : 0.4391903281211853 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6551121391355992 VALIDATION ACCURACY : 80.15625\n",
      "LOSS for EPOCH 15 BATCH 400/1155 TRAIN LOSS : 1.1028587818145752 TRAIN ACCURACY : 75.0 VALIDATION LOSS : 0.6838629182428122 VALIDATION ACCURACY : 79.84375\n",
      "LOSS for EPOCH 15 BATCH 500/1155 TRAIN LOSS : 0.9449098706245422 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6585449354723096 VALIDATION ACCURACY : 79.375\n",
      "LOSS for EPOCH 15 BATCH 600/1155 TRAIN LOSS : 0.8650364875793457 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6603872757405043 VALIDATION ACCURACY : 80.46875\n",
      "LOSS for EPOCH 15 BATCH 700/1155 TRAIN LOSS : 0.6341906189918518 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6626611208543182 VALIDATION ACCURACY : 81.09375\n",
      "LOSS for EPOCH 15 BATCH 800/1155 TRAIN LOSS : 0.8766107559204102 TRAIN ACCURACY : 87.5 VALIDATION LOSS : 0.6640645913779736 VALIDATION ACCURACY : 80.78125\n",
      "LOSS for EPOCH 15 BATCH 900/1155 TRAIN LOSS : 0.8328056335449219 TRAIN ACCURACY : 62.5 VALIDATION LOSS : 0.6683690279722214 VALIDATION ACCURACY : 80.0\n",
      "LOSS for EPOCH 15 BATCH 1000/1155 TRAIN LOSS : 1.0512101650238037 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6769648564513773 VALIDATION ACCURACY : 79.53125\n",
      "LOSS for EPOCH 15 BATCH 1100/1155 TRAIN LOSS : 1.074468970298767 TRAIN ACCURACY : 68.75 VALIDATION LOSS : 0.6490595404058694 VALIDATION ACCURACY : 81.09375\n",
      "---------------------------------------EPOCH 15-------------------------------------------\n",
      "Loss for EPOCH 15  TRAIN LOSS : 0.7703906924306573 TRAIN ACCURACY : 77.71825396825396\n",
      "VALIDATION LOSS for EPOCH 15 : 0.6638857441149991 VALIDATION ACCURACY : 80.2840909090909\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    for batch_idx , (x ,y) in enumerate(train_dataloader):\n",
    "        model.train() # Setting mode to train\n",
    "        optimizer.zero_grad()\n",
    "        x , y = x.to(device) , y.to(device)\n",
    "        y_pred = model(x).to(device)\n",
    "        \n",
    "        # Calculating Loss\n",
    "        loss = criterion(y_pred,y.reshape(x.shape[0]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history[0].append(float(loss.detach()))\n",
    "        \n",
    "        #Calaculating Accuracy\n",
    "        correct = 0\n",
    "        y_pred = y_pred.cpu().detach().numpy().tolist()\n",
    "        y = y.cpu().detach().numpy().tolist()\n",
    "        for i in range(x.shape[0]):\n",
    "            n = 0\n",
    "            n = y_pred[i].index(max(y_pred[i]))\n",
    "            if n == y[i][0]:\n",
    "                correct = correct + 1\n",
    "        accuracy_history[0].append((correct/x.shape[0])*100)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            # Printing Log\n",
    "            print(f'LOSS for EPOCH {e+1} BATCH {batch_idx}/{train_n_minibatches} TRAIN LOSS : {loss_history[0][-1]}',end = ' ')\n",
    "            print(f'TRAIN ACCURACY : {accuracy_history[0][-1]}',end = ' ')\n",
    "            with torch.no_grad():\n",
    "                # Calculating loss and accuracy for validation\n",
    "                model.eval()\n",
    "                for _batch_idx_ , (x ,y) in enumerate(validation_dataloader):\n",
    "                    x , y = x.to(device) , y.to(device)\n",
    "                    y_pred = model(x).to(device)\n",
    "                    validation_loss = criterion(y_pred,y.reshape(x.shape[0]))\n",
    "                    loss_history[1].append(float(validation_loss.detach()))\n",
    "                    \n",
    "                    correct = 0\n",
    "                    y_pred = y_pred.cpu().detach().numpy().tolist()\n",
    "                    y = y.cpu().detach().numpy().tolist()      \n",
    "                    for i in range(x.shape[0]):\n",
    "                        n = 0\n",
    "                        n = y_pred[i].index(max(y_pred[i]))\n",
    "                        if n == y[i][0]:\n",
    "                            correct = correct + 1\n",
    "                    accuracy_history[1].append((correct/x.shape[0])*100)\n",
    "                        \n",
    "                    \n",
    "                print(f'VALIDATION LOSS : {sum(loss_history[1][-1:-validation_n_minibatches-1:-1])/validation_n_minibatches}',end = ' ')\n",
    "                print(f'VALIDATION ACCURACY : {sum(accuracy_history[1][-1:-validation_n_minibatches-1:-1])/validation_n_minibatches}')\n",
    "    \n",
    "    # Saving the model progress\n",
    "    torch.save(model.state_dict(),'saved_model/resnet50_v1')\n",
    "    \n",
    "    #Log for e+1th epoch\n",
    "    print(f'---------------------------------------EPOCH {e+1}-------------------------------------------')\n",
    "    print(f'Loss for EPOCH {e+1}  TRAIN LOSS : {sum(loss_history[0][-1:-train_n_minibatches-1:-1])/train_n_minibatches}',end = ' ')\n",
    "    print(f'TRAIN ACCURACY : {sum(accuracy_history[0][-1:-train_n_minibatches-1:-1])/train_n_minibatches}')\n",
    "    n_validation_losses = int(train_n_minibatches/100)*validation_n_minibatches\n",
    "    print(f'VALIDATION LOSS for EPOCH {e+1} : {sum(loss_history[1][-1:-1*n_validation_losses-1:-1])/n_validation_losses}',end = ' ')\n",
    "    print(f'VALIDATION ACCURACY : {sum(accuracy_history[1][-1:-1*n_validation_losses-1:-1])/n_validation_losses}')\n",
    "    print('---------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0e8db",
   "metadata": {},
   "source": [
    "### 5.Plotting Graphs<a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b9797",
   "metadata": {},
   "source": [
    "#### 1.Plotting Loss vs Epoch<a class=\"anchor\" id=\"5.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee1fb104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "c:\\python\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Loss per epoch\n",
    "loss_per_epoch = [[],[]]\n",
    "for i in range(epoch):\n",
    "    temp = 0\n",
    "    for j in loss_history[0][i*train_n_minibatches:(i+1)*train_n_minibatches]:\n",
    "        temp = temp + j\n",
    "    loss_per_epoch[0].append(temp/train_n_minibatches)\n",
    "    temp = 0\n",
    "    for j in loss_history[1][i*n_validation_losses:(i+1)*n_validation_losses]:\n",
    "        temp = temp + j\n",
    "    loss_per_epoch[1].append(temp/n_validation_losses)    \n",
    "\n",
    "sns.lineplot(range(len(loss_per_epoch[0])),loss_per_epoch[0])\n",
    "sns.lineplot(range(len(loss_per_epoch[1])),loss_per_epoch[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b060154",
   "metadata": {},
   "source": [
    "#### 2.Plotting Accuracy vs Epoch<a class=\"anchor\" id=\"5.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c5d076e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n",
      "c:\\python\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting Accuracy per epoch\n",
    "accuracy_per_epoch = [[],[]]\n",
    "for i in range(epoch):\n",
    "    temp = 0\n",
    "    for j in accuracy_history[0][i*train_n_minibatches:(i+1)*train_n_minibatches]:\n",
    "        temp = temp + j\n",
    "    accuracy_per_epoch[0].append(temp/train_n_minibatches)\n",
    "    temp = 0\n",
    "    for j in accuracy_history[1][i*n_validation_losses:(i+1)*n_validation_losses]:\n",
    "        temp = temp + j\n",
    "    accuracy_per_epoch[1].append(temp/n_validation_losses)    \n",
    "\n",
    "sns.lineplot(range(len(accuracy_per_epoch[0])),accuracy_per_epoch[0])\n",
    "sns.lineplot(range(len(accuracy_per_epoch[1])),accuracy_per_epoch[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93802e0d",
   "metadata": {},
   "source": [
    "### 6.Loading and Testing<a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929fe8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the saved model\n",
    "model = models.resnet50(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, n_classes)\n",
    "model.load_state_dict(torch.load('saved_model/resnet50_v1', map_location='cpu'))\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6db7ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [16, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [16, 64, 112, 112]             128\n",
      "              ReLU-3         [16, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [16, 64, 56, 56]               0\n",
      "            Conv2d-5           [16, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [16, 64, 56, 56]             128\n",
      "              ReLU-7           [16, 64, 56, 56]               0\n",
      "            Conv2d-8           [16, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [16, 64, 56, 56]             128\n",
      "             ReLU-10           [16, 64, 56, 56]               0\n",
      "           Conv2d-11          [16, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [16, 256, 56, 56]             512\n",
      "           Conv2d-13          [16, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [16, 256, 56, 56]             512\n",
      "             ReLU-15          [16, 256, 56, 56]               0\n",
      "       Bottleneck-16          [16, 256, 56, 56]               0\n",
      "           Conv2d-17           [16, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [16, 64, 56, 56]             128\n",
      "             ReLU-19           [16, 64, 56, 56]               0\n",
      "           Conv2d-20           [16, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [16, 64, 56, 56]             128\n",
      "             ReLU-22           [16, 64, 56, 56]               0\n",
      "           Conv2d-23          [16, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [16, 256, 56, 56]             512\n",
      "             ReLU-25          [16, 256, 56, 56]               0\n",
      "       Bottleneck-26          [16, 256, 56, 56]               0\n",
      "           Conv2d-27           [16, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [16, 64, 56, 56]             128\n",
      "             ReLU-29           [16, 64, 56, 56]               0\n",
      "           Conv2d-30           [16, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [16, 64, 56, 56]             128\n",
      "             ReLU-32           [16, 64, 56, 56]               0\n",
      "           Conv2d-33          [16, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [16, 256, 56, 56]             512\n",
      "             ReLU-35          [16, 256, 56, 56]               0\n",
      "       Bottleneck-36          [16, 256, 56, 56]               0\n",
      "           Conv2d-37          [16, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [16, 128, 56, 56]             256\n",
      "             ReLU-39          [16, 128, 56, 56]               0\n",
      "           Conv2d-40          [16, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [16, 128, 28, 28]             256\n",
      "             ReLU-42          [16, 128, 28, 28]               0\n",
      "           Conv2d-43          [16, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [16, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [16, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [16, 512, 28, 28]           1,024\n",
      "             ReLU-47          [16, 512, 28, 28]               0\n",
      "       Bottleneck-48          [16, 512, 28, 28]               0\n",
      "           Conv2d-49          [16, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [16, 128, 28, 28]             256\n",
      "             ReLU-51          [16, 128, 28, 28]               0\n",
      "           Conv2d-52          [16, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [16, 128, 28, 28]             256\n",
      "             ReLU-54          [16, 128, 28, 28]               0\n",
      "           Conv2d-55          [16, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [16, 512, 28, 28]           1,024\n",
      "             ReLU-57          [16, 512, 28, 28]               0\n",
      "       Bottleneck-58          [16, 512, 28, 28]               0\n",
      "           Conv2d-59          [16, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [16, 128, 28, 28]             256\n",
      "             ReLU-61          [16, 128, 28, 28]               0\n",
      "           Conv2d-62          [16, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [16, 128, 28, 28]             256\n",
      "             ReLU-64          [16, 128, 28, 28]               0\n",
      "           Conv2d-65          [16, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [16, 512, 28, 28]           1,024\n",
      "             ReLU-67          [16, 512, 28, 28]               0\n",
      "       Bottleneck-68          [16, 512, 28, 28]               0\n",
      "           Conv2d-69          [16, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [16, 128, 28, 28]             256\n",
      "             ReLU-71          [16, 128, 28, 28]               0\n",
      "           Conv2d-72          [16, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [16, 128, 28, 28]             256\n",
      "             ReLU-74          [16, 128, 28, 28]               0\n",
      "           Conv2d-75          [16, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [16, 512, 28, 28]           1,024\n",
      "             ReLU-77          [16, 512, 28, 28]               0\n",
      "       Bottleneck-78          [16, 512, 28, 28]               0\n",
      "           Conv2d-79          [16, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [16, 256, 28, 28]             512\n",
      "             ReLU-81          [16, 256, 28, 28]               0\n",
      "           Conv2d-82          [16, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [16, 256, 14, 14]             512\n",
      "             ReLU-84          [16, 256, 14, 14]               0\n",
      "           Conv2d-85         [16, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [16, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [16, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [16, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [16, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [16, 1024, 14, 14]               0\n",
      "           Conv2d-91          [16, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [16, 256, 14, 14]             512\n",
      "             ReLU-93          [16, 256, 14, 14]               0\n",
      "           Conv2d-94          [16, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [16, 256, 14, 14]             512\n",
      "             ReLU-96          [16, 256, 14, 14]               0\n",
      "           Conv2d-97         [16, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [16, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [16, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [16, 1024, 14, 14]               0\n",
      "          Conv2d-101          [16, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [16, 256, 14, 14]             512\n",
      "            ReLU-103          [16, 256, 14, 14]               0\n",
      "          Conv2d-104          [16, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [16, 256, 14, 14]             512\n",
      "            ReLU-106          [16, 256, 14, 14]               0\n",
      "          Conv2d-107         [16, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [16, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [16, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [16, 1024, 14, 14]               0\n",
      "          Conv2d-111          [16, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [16, 256, 14, 14]             512\n",
      "            ReLU-113          [16, 256, 14, 14]               0\n",
      "          Conv2d-114          [16, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [16, 256, 14, 14]             512\n",
      "            ReLU-116          [16, 256, 14, 14]               0\n",
      "          Conv2d-117         [16, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [16, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [16, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [16, 1024, 14, 14]               0\n",
      "          Conv2d-121          [16, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [16, 256, 14, 14]             512\n",
      "            ReLU-123          [16, 256, 14, 14]               0\n",
      "          Conv2d-124          [16, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [16, 256, 14, 14]             512\n",
      "            ReLU-126          [16, 256, 14, 14]               0\n",
      "          Conv2d-127         [16, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [16, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [16, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [16, 1024, 14, 14]               0\n",
      "          Conv2d-131          [16, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [16, 256, 14, 14]             512\n",
      "            ReLU-133          [16, 256, 14, 14]               0\n",
      "          Conv2d-134          [16, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [16, 256, 14, 14]             512\n",
      "            ReLU-136          [16, 256, 14, 14]               0\n",
      "          Conv2d-137         [16, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [16, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [16, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [16, 1024, 14, 14]               0\n",
      "          Conv2d-141          [16, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [16, 512, 14, 14]           1,024\n",
      "            ReLU-143          [16, 512, 14, 14]               0\n",
      "          Conv2d-144            [16, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [16, 512, 7, 7]           1,024\n",
      "            ReLU-146            [16, 512, 7, 7]               0\n",
      "          Conv2d-147           [16, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [16, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [16, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [16, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [16, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [16, 2048, 7, 7]               0\n",
      "          Conv2d-153            [16, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [16, 512, 7, 7]           1,024\n",
      "            ReLU-155            [16, 512, 7, 7]               0\n",
      "          Conv2d-156            [16, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [16, 512, 7, 7]           1,024\n",
      "            ReLU-158            [16, 512, 7, 7]               0\n",
      "          Conv2d-159           [16, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [16, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [16, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [16, 2048, 7, 7]               0\n",
      "          Conv2d-163            [16, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [16, 512, 7, 7]           1,024\n",
      "            ReLU-165            [16, 512, 7, 7]               0\n",
      "          Conv2d-166            [16, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [16, 512, 7, 7]           1,024\n",
      "            ReLU-168            [16, 512, 7, 7]               0\n",
      "          Conv2d-169           [16, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [16, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [16, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [16, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [16, 2048, 1, 1]               0\n",
      "          Linear-174                  [16, 120]         245,880\n",
      "================================================================\n",
      "Total params: 23,753,912\n",
      "Trainable params: 23,753,912\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 9.19\n",
      "Forward/backward pass size (MB): 4584.83\n",
      "Params size (MB): 90.61\n",
      "Estimated Total Size (MB): 4684.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Displaying the model summary\n",
    "from torchsummary import summary\n",
    "summary(model, (3,224,224), batch_size=16, device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3b9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_history = []\n",
    "test_accuracy_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050de6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS : 0.6438697055783323  ACCURACY : 80.26881720430107\n"
     ]
    }
   ],
   "source": [
    "#Testing the model on test dataset\n",
    "model = model.to('cpu')\n",
    "model.eval()\n",
    "for _batch_idx_ , (x ,y) in enumerate(test_dataloader):\n",
    "    y_pred = model(x)\n",
    "    test_loss = criterion(y_pred,y.reshape(x.shape[0]))\n",
    "    test_loss_history.append(float(test_loss.detach()))\n",
    "    correct = 0\n",
    "    y_pred = y_pred.detach().numpy().tolist()\n",
    "    y = y.detach().numpy().tolist()      \n",
    "    for i in range(x.shape[0]):\n",
    "        n = 0\n",
    "        n = y_pred[i].index(max(y_pred[i]))\n",
    "        if n == y[i][0]:\n",
    "            correct = correct + 1\n",
    "    test_accuracy_history.append((correct/len(y))*100)\n",
    "                        \n",
    "print(f'LOSS : {sum(test_loss_history)/len(test_loss_history)}  ACCURACY : {sum(test_accuracy_history)/len(test_accuracy_history)}')                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4811f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
